## Details

虽然是非常基本的 demo，但是也有可以讨论的地方。

搭建一个机器学习基本的流程
- step1: 设计并实现模型
- step2: 训练模型
- step3: 模型预估
- 其中step1定了之后，后续两步是循环的过程。
- step1不用亲力亲为，借助工具即可

### 模型设计

这个重要，说一下。根据深度学习的数学这本书里的内容，对于0/1识别这个
demo。正解的设计是0-1有两个输出神经单元，为什么当前的case只有一个输出？

- 核心就是大家解决的不是一个问题。
- 0-1分类，本质是识别 0-1.所以需要feature detection或者说pattern recognition。那么正解个数一定是最终需要识别的数据个数。
- 正负数分类，本质并不是识别。所以输出神经单元不用枚举识别状态。
  - 它的核心是找一个超平面，可以把 0-1分开。
  - 超平面一定是一个多元线性方程。既然是一个方程，那么输出一定是一个。
  - 所以，只有一个输出单元。
  - 模型输出的值，其实就是代入x后，在超平面上下的位置。

### 结果

书上的demo，只训练 10 次。显然，我执行的时候，没收敛。调整到 1000 次，看起来也没收敛。不过已经能明显的分开了。

- 第一层为什么input_dim==1，这是因为就一个特征。
- 明明只有 10 个样本，为什么能训练 1000 次。

这个也是有意思问题，更新到另一个文档里。
我说一个我不解的地方：
- 根据sgd算法，我们知道cost function每次都要找西江最快的方向，迈一小步。
- 那么同样的sample,岂不是要回到同一个地方，再迈一小步。这不重复了？

这里的理解就错了。
- 当时深度学习的数学为了讲清楚sgd，做了个类比。类比了二维的点。
- 为了说明在不同的坐标，向前迈步子的方向都不一致。
- 我理解错了，理解成了坐标，代表了样本。其实不是，坐标代表了不同的 CT 在图上的位置。
- 还是得回归cost function来理解。
  - 有一个样本，就是c1
  - 有 2 个样本，即使是 2 个同样的样本。到达c2，跟c1也不是同一个一个关于(p,q)的函数
    - c1 = 0.5 * (45.5 - (153.p + q)^2)
    - c2 = 0.5 * (45.5 - (153.p + q)^2) + 0.5 * (45.5 - (153.p + q)^2)
    - c1/c2其实代表的是不同的坐标点。这是正确的理解。
  - 所以，训练次数，可以多余sample个数。

其实上面的理解也不对。
- 深度学习的数学当中，类比的坐标点，其实是当前的参数值。
- 每进行一次updates，更新一次。从而有了新的坐标。是这个意思。

再说一点，参数的学习。或者就是机器学习，底层是数理统计。可能说参数有点难理解，
我们就理解一个本的成绩，需要计算平均分。一个道理。都是利用样本进行数理统计并计算。
- 一个班的平均分，如果计算每个同学的，那就是一般梯度下降的方式。
- 如果采样一部分同学的，那就是sgd的方式。
- 如果按这种理解，反复epoch相同样本，其实并不会让参数训练变的更准。但是反复epoch这个事，是可以做的。